# Workflow Overview â€“ Automated Resumeâ€“Job Matching & Cover Letter Generator

This document provides a **complete, end-to-end explanation** of the workflow, describing **what each node does**, **how data flows**, and **why specific design choices were made**.  
The goal of this automation is to reliably discover relevant jobs, enrich them using AI, and store results **without duplication**, even across repeated runs.

---

## ğŸ¯ Objective

To build a **restart-safe, configuration-driven automation** that:
- Searches LinkedIn jobs dynamically
- Evaluates job relevance against a resume using an LLM
- Generates personalized cover letters
- Stores results in Google Sheets
- Avoids redundant entries across executions

---

## ğŸ§  High-Level Architecture

The workflow is composed of five logical layers:

1. **Configuration & Inputs**
2. **Resume Ingestion**
3. **Job Discovery**
4. **Per-Job Processing & AI Enrichment**
5. **Deduplication & Persistence**

Each layer is intentionally isolated to keep the workflow scalable, debuggable, and safe to re-run.

---

## 1ï¸âƒ£ Configuration & Control Layer

### ğŸ”¹ Google Sheets â€“ Filter Sheet
This sheet acts as the **control panel** for the entire workflow.

It defines:
- Keywords
- Location
- Experience level
- Job type
- Remote preference
- Easy Apply filter

**Why this matters:**  
Search behavior can be modified without touching the workflow or code, making the system flexible and user-friendly.

---

## 2ï¸âƒ£ Resume Ingestion Layer

### ğŸ”¹ Download File (Google Drive)
- Downloads the candidateâ€™s resume (PDF) from Google Drive at runtime
- Decouples resume content from the workflow itself

### ğŸ”¹ Extract from File
- Converts the PDF resume into plain text
- Prepares the resume for semantic analysis by the LLM

**Why this matters:**  
The resume is treated as dynamic input, allowing easy replacement or reuse across different runs or candidates.

---

## 3ï¸âƒ£ Job Discovery Layer

### ğŸ”¹ JavaScript â€“ LinkedIn Search URL Builder
- Transforms filter-sheet values into valid LinkedIn search query parameters
- Handles mapping of:
  - Experience levels
  - Job types
  - Remote preferences

**Why this matters:**  
LinkedInâ€™s query syntax is not user-friendly. This node acts as a translation layer between human-readable filters and machine-readable URLs.

---

### ğŸ”¹ HTTP Request â€“ LinkedIn Search Page
- Fetches raw HTML for LinkedIn job search results

### ğŸ”¹ HTML â€“ Extract Job Links
- Parses the search page
- Extracts individual job posting URLs

### ğŸ”¹ Split Out
- Converts a list of job URLs into **individual items**

---

## 4ï¸âƒ£ Job Processing & AI Enrichment Layer

### ğŸ”¹ Loop Over Items
- Iterates over each job URL independently
- Ensures:
  - Isolation between jobs
  - Retry safety
  - Scalability

**Why this matters:**  
Each job is processed as a standalone unit, preventing partial failures from affecting other jobs.

---

### ğŸ”¹ HTTP Request â€“ Job Detail Page
- Fetches HTML for a single job posting

### ğŸ”¹ HTML â€“ Extract Job Attributes
Extracts structured job data:
- Job title
- Company
- Location
- Job description
- Apply link

### ğŸ”¹ Edit Fields
- Normalizes extracted fields
- Aligns key names and formats

---

### ğŸ”¹ AI Agent (LLM)
**Inputs:**
- Resume text (from Google Drive)
- Job description (from LinkedIn)

**Outputs:**
- Resumeâ€“job match score
- Personalized cover letter

**Why this matters:**  
This replaces keyword matching with **semantic reasoning**, enabling more meaningful job relevance scoring and tailored content generation.

---

### ğŸ”¹ Edit Fields (Final Assembly)
- Combines:
  - Job metadata
  - Match score
  - Generated cover letter
- Aligns the final data structure with the output sheet schema

---

## 5ï¸âƒ£ Deduplication & Persistence Layer (Key Feature)

### ğŸ”¹ Get Existing Results (Google Sheets â€“ Result Sheet)
- Reads previously processed job entries
- Configured with **â€œAlways Output Dataâ€** to prevent execution blocking when the sheet is empty
- Used strictly as **reference data**, not as a driver of execution

**Why this matters:**  
This enables the workflow to be **idempotent** â€” safe to re-run without duplicating data.

---

### ğŸ”¹ Deduplicate Job (Code Node)
- Normalizes Apply Links
- Compares the current job against historical entries
- Drops duplicates silently
- Allows only genuinely new jobs to proceed

**Design decision:**  
Deduplication is done via a Code node instead of an IF node to:
- Avoid execution-order issues
- Handle empty reference data safely
- Normalize URLs reliably

---

### ğŸ”¹ Append Row (Google Sheets â€“ Result Sheet)
- Writes only new, enriched job entries
- Stores:
  - Title
  - Company
  - Location
  - Apply Link
  - Match score
  - Job description
  - AI-generated cover letter

---

## ğŸ” Execution Guarantees

- **Restart-safe**: Workflow can be stopped and re-run without side effects
- **Duplicate-proof**: Previously processed jobs are never re-added
- **Config-driven**: Changing inputs requires no workflow edits
- **Scalable**: Handles multiple jobs per execution via looping

---

## ğŸ§  Why This Workflow Stands Out

- Treats Google Sheets as both **control plane and datastore**
- Separates **reference data** from **execution flow**
- Uses AI as a reasoning layer, not just text generation
- Designed with real-world automation constraints in mind

---

## ğŸ Summary

This workflow demonstrates how n8n can be used to orchestrate a robust, AI-powered automation that combines web scraping, document processing, semantic analysis, and stateful deduplication into a single cohesive system.

It is intentionally designed to behave like a production pipeline rather than a one-off script.
